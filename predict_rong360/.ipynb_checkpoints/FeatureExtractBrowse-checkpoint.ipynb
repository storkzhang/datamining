{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T06:09:18.462431Z",
     "start_time": "2018-11-18T06:09:18.039779Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "# Author:                   \n",
    "# Date: Nov.8, 2018         \n",
    "# Version: v3            \n",
    "# Notice:      \n",
    "# 1. FeatureExtract\n",
    "# 2. directories structure\n",
    "#   code/\n",
    "#   model/\n",
    "#   feature/\n",
    "#   answer/\n",
    "#   train/\n",
    "#   A/\n",
    "#   MAKE SURE ALL OF THE DIR IS EXISTED!!!\n",
    "# 3. 特征均存储在../feature目录\n",
    "# \n",
    "# 4. 最终的特征汇总在features_all_train_test_20181106.csv\n",
    "#    包含所有train和A的数据\n",
    "#########################################################\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# common prefix\n",
    "TRAIN_PREFIX = \"../train/\"\n",
    "TEST_PREFIX = \"../B/\"\n",
    "FEATURE_PREFIX = \"../feature/\"\n",
    "\n",
    "# File Encoder\n",
    "FILE_ENCODER = \"utf-8\"\n",
    "\n",
    "#Train-dataset\n",
    "BANK_DETAIL_TRAIN_FILE = TRAIN_PREFIX + \"bank_detail_train.csv\"\n",
    "BILL_DETAIL_TRAIN_FILE = TRAIN_PREFIX + \"bill_detail_train.csv\"\n",
    "BROWSE_HISTORY_TRAIN_FILE = TRAIN_PREFIX +\"browse_history_train.csv\"\n",
    "LOANTIME_TRAIN_FILE = TRAIN_PREFIX + \"loantime_train.csv\"\n",
    "OVERDUE_TRAIN_FILE = TRAIN_PREFIX + \"overdue_train.csv\"\n",
    "USERINFO_TRAIN_FILE = TRAIN_PREFIX + \"userinfo_train.csv\"\n",
    "\n",
    "#Test-dataset\n",
    "BANK_DETAIL_TEST_FILE = TEST_PREFIX + \"bank_detail_B.csv\"\n",
    "BILL_DETAIL_TEST_FILE = TEST_PREFIX + \"bill_detail_B.csv\"\n",
    "BROWSE_HISTORY_TEST_FILE = TEST_PREFIX + \"browse_history_B.csv\"\n",
    "LOANTIME_TEST_FILE = TEST_PREFIX + \"loantime_B.csv\"\n",
    "USERINFO_TEST_FILE = TEST_PREFIX + \"userinfo_B.csv\"\n",
    "USERID_TO_PREDICT_FILE = FEATURE_PREFIX + \"to_predict_userid.csv\" #因为A目录没有写入权限\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T06:09:18.930482Z",
     "start_time": "2018-11-18T06:09:18.584841Z"
    }
   },
   "outputs": [],
   "source": [
    "def constructBrowseDetailFeature(loantime_file, browse_detail_file, output_file):\n",
    "    loantime = pd.read_csv(loantime_file,header=0,names=['用户标识','放款时间'])\n",
    "    loantime['放款时间']=loantime['放款时间']//86400\n",
    "    browse_detail = pd.read_csv(browse_detail_file,header=0,\n",
    "                        names=['用户标识','浏览时间','浏览行为数据','浏览子行为编号'])\n",
    "    browse_detail['浏览时间']=browse_detail['浏览时间']//86400\n",
    "    \n",
    "    feature=loantime\n",
    "    d= pd.merge(browse_detail, loantime,how='left', on = \"用户标识\")\n",
    "\n",
    "    #----------------------------------------放款前特征统计------------------------------------------#\n",
    "    #统计放款前用户浏览子行为总数以及浏览行为数据总和\n",
    "    gb=d[(d['浏览时间']<=d['放款时间'])].groupby([\"用户标识\"],as_index=False)\n",
    "    x1=gb['浏览行为数据'].agg({'放款前浏览行为数据sum' : 'sum','放款前浏览行为数据max' : 'max','放款前浏览行为数据mean' : 'mean'\n",
    "                        ,'放款前浏览行为数据min' : 'min','放款前浏览行为数据std' : 'std','放款前浏览行为数据var' : 'var'})\n",
    "    xx=gb['浏览子行为编号'].apply(lambda x:np.unique(x).size)\n",
    "    x2=gb['浏览子行为编号'].agg({'放款前浏览子行为编号count' : 'count'})\n",
    "    x2['放款前浏览子行为编号计数（去重）']=xx\n",
    "\n",
    "    feature=pd.merge(feature, x1,how='left', on = \"用户标识\")\n",
    "    feature=pd.merge(feature, x2,how='left', on = \"用户标识\")\n",
    "    \n",
    "    \n",
    "#     gb = d[['用户标识','浏览时间']].groupby([\"用户标识\"], as_index = False)\n",
    "#     timestamp_agg = gb.agg(['sum','max','min','mean','median','var','count']).reset_index()\n",
    "#     feature = pd.merge(feature, timestamp_agg , how = 'left', on = '用户标识')\n",
    "\n",
    "    #统计放款前用户浏览子行为个类别统计信息\n",
    "    d=pd.get_dummies(d,columns=['浏览子行为编号'])#22919547 rows × 14 columns\n",
    "    gb=d[(d['浏览时间']<=d['放款时间'])].groupby([\"用户标识\"],as_index=False)\n",
    "    x1=gb['浏览子行为编号_1'].agg({'放款前浏览子行为编号_1' : 'sum'})\n",
    "    x2=gb['浏览子行为编号_2'].agg({'放款前浏览子行为编号_2' : 'sum'})\n",
    "    x3=gb['浏览子行为编号_3'].agg({'放款前浏览子行为编号_3' : 'sum'})\n",
    "    x4=gb['浏览子行为编号_4'].agg({'放款前浏览子行为编号_4' : 'sum'})\n",
    "    x5=gb['浏览子行为编号_5'].agg({'放款前浏览子行为编号_5' : 'sum'})\n",
    "    x6=gb['浏览子行为编号_6'].agg({'放款前浏览子行为编号_6' : 'sum'})\n",
    "    x7=gb['浏览子行为编号_7'].agg({'放款前浏览子行为编号_7' : 'sum'})\n",
    "    x8=gb['浏览子行为编号_8'].agg({'放款前浏览子行为编号_8' : 'sum'})\n",
    "    x9=gb['浏览子行为编号_9'].agg({'放款前浏览子行为编号_9' : 'sum'})\n",
    "    x10=gb['浏览子行为编号_10'].agg({'放款前浏览子行为编号_10' : 'sum'})\n",
    "    x11=gb['浏览子行为编号_11'].agg({'放款前浏览子行为编号_11' : 'sum'})\n",
    "\n",
    "    feature=pd.merge(feature, x1,how='left', on = \"用户标识\")\n",
    "    feature=pd.merge(feature, x2,how='left', on = \"用户标识\")\n",
    "    feature=pd.merge(feature, x3,how='left', on = \"用户标识\")\n",
    "    feature=pd.merge(feature, x4,how='left', on = \"用户标识\")\n",
    "    feature=pd.merge(feature, x5,how='left', on = \"用户标识\")\n",
    "    feature=pd.merge(feature, x6,how='left', on = \"用户标识\")\n",
    "    feature=pd.merge(feature, x7,how='left', on = \"用户标识\")\n",
    "    feature=pd.merge(feature, x8,how='left', on = \"用户标识\")\n",
    "    feature=pd.merge(feature, x9,how='left', on = \"用户标识\")\n",
    "    feature=pd.merge(feature, x10,how='left', on = \"用户标识\")\n",
    "    feature=pd.merge(feature, x11,how='left', on = \"用户标识\")\n",
    "\n",
    "    d= pd.merge(browse_detail, loantime,how='left', on = \"用户标识\")#22919547 rows × 5 columns\n",
    "    \n",
    "    #----------------------------------------放款后特征统计------------------------------------------#\n",
    "    #统计放款后用户浏览子行为总数以及浏览行为数据总和\n",
    "    gb=d[(d['浏览时间']>d['放款时间'])].groupby([\"用户标识\"],as_index=False)\n",
    "    x1=gb['浏览行为数据'].agg({'放款后浏览行为数据sum' : 'sum','放款后浏览行为数据max' : 'max','放款后浏览行为数据mean' : 'mean'\n",
    "                         ,'放款后浏览行为数据min' : 'min','放款后浏览行为数据std' : 'std','放款后浏览行为数据var' : 'var'})\n",
    "    xx=gb['浏览子行为编号'].apply(lambda x:np.unique(x).size)\n",
    "    x2=gb['浏览子行为编号'].agg({'放款后浏览子行为编号count' : 'count'})\n",
    "    x2['放款后浏览子行为编号计数（去重）']=xx\n",
    "\n",
    "    feature=pd.merge(feature, x1,how='left', on = \"用户标识\")\n",
    "    feature=pd.merge(feature, x2,how='left', on = \"用户标识\")\n",
    "    \n",
    "#     gb = d[['用户标识','浏览时间']].groupby([\"用户标识\"], as_index = False)\n",
    "#     timestamp_agg = gb.agg(['sum','max','min','mean','median','var','count']).reset_index()\n",
    "#     feature = pd.merge(feature, timestamp_agg , how = 'left', on = '用户标识')\n",
    "\n",
    "    #统计放款前用户浏览子行为个类别统计信息\n",
    "    d=pd.get_dummies(d,columns=['浏览子行为编号'])#22919547 rows × 14 columns\n",
    "    gb=d[(d['浏览时间']<=d['放款时间'])].groupby([\"用户标识\"],as_index=False)\n",
    "    x1=gb['浏览子行为编号_1'].agg({'放款后浏览子行为编号_1' : 'sum'})\n",
    "    x2=gb['浏览子行为编号_2'].agg({'放款后浏览子行为编号_2' : 'sum'})\n",
    "    x3=gb['浏览子行为编号_3'].agg({'放款后浏览子行为编号_3' : 'sum'})\n",
    "    x4=gb['浏览子行为编号_4'].agg({'放款后浏览子行为编号_4' : 'sum'})\n",
    "    x5=gb['浏览子行为编号_5'].agg({'放款后浏览子行为编号_5' : 'sum'})\n",
    "    x6=gb['浏览子行为编号_6'].agg({'放款后浏览子行为编号_6' : 'sum'})\n",
    "    x7=gb['浏览子行为编号_7'].agg({'放款后浏览子行为编号_7' : 'sum'})\n",
    "    x8=gb['浏览子行为编号_8'].agg({'放款后浏览子行为编号_8' : 'sum'})\n",
    "    x9=gb['浏览子行为编号_9'].agg({'放款后浏览子行为编号_9' : 'sum'})\n",
    "    x10=gb['浏览子行为编号_10'].agg({'放款后浏览子行为编号_10' : 'sum'})\n",
    "    x11=gb['浏览子行为编号_11'].agg({'放款后浏览子行为编号_11' : 'sum'})\n",
    "\n",
    "    feature=pd.merge(feature, x1,how='left', on = \"用户标识\")\n",
    "    feature=pd.merge(feature, x2,how='left', on = \"用户标识\")\n",
    "    feature=pd.merge(feature, x3,how='left', on = \"用户标识\")\n",
    "    feature=pd.merge(feature, x4,how='left', on = \"用户标识\")\n",
    "    feature=pd.merge(feature, x5,how='left', on = \"用户标识\")\n",
    "    feature=pd.merge(feature, x6,how='left', on = \"用户标识\")\n",
    "    feature=pd.merge(feature, x7,how='left', on = \"用户标识\")\n",
    "    feature=pd.merge(feature, x8,how='left', on = \"用户标识\")\n",
    "    feature=pd.merge(feature, x9,how='left', on = \"用户标识\")\n",
    "    feature=pd.merge(feature, x10,how='left', on = \"用户标识\")\n",
    "    feature=pd.merge(feature, x11,how='left', on = \"用户标识\")\n",
    "    print(feature.shape)\n",
    "    \n",
    "    \n",
    "    d= browse_detail[['用户标识','浏览时间']]\n",
    "    gb = d.groupby([\"用户标识\"], as_index = False)\n",
    "    timestamp_agg = gb.agg(['sum','max','min','mean','median','var','count']).reset_index()\n",
    "    feature = pd.merge(feature, timestamp_agg , how = 'left', on = '用户标识')\n",
    "    print(feature.shape)\n",
    "\n",
    "    feature.to_csv(output_file,index=None,encoding=FILE_ENCODER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正式代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-18T06:09:19.250Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'../train/loantime_train.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-20f7379971f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"begin\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m constructBrowseDetailFeature(LOANTIME_TRAIN_FILE, BROWSE_HISTORY_TRAIN_FILE, \n\u001b[0;32m----> 3\u001b[0;31m                            \"../feature/browse_detail_train20181106.csv\")\n\u001b[0m\u001b[1;32m      4\u001b[0m constructBrowseDetailFeature(LOANTIME_TEST_FILE, BROWSE_HISTORY_TEST_FILE, \n\u001b[1;32m      5\u001b[0m                            \"../feature/browse_detail_test20181106.csv\")\n",
      "\u001b[0;32m<ipython-input-2-2ff70953930b>\u001b[0m in \u001b[0;36mconstructBrowseDetailFeature\u001b[0;34m(loantime_file, browse_detail_file, output_file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconstructBrowseDetailFeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloantime_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrowse_detail_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mloantime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloantime_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'用户标识'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'放款时间'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mloantime\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'放款时间'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloantime\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'放款时间'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m86400\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     browse_detail = pd.read_csv(browse_detail_file,header=0,\n\u001b[1;32m      5\u001b[0m                         names=['用户标识','浏览时间','浏览行为数据','浏览子行为编号'])\n",
      "\u001b[0;32m/data/soft/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/soft/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/soft/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/soft/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/soft/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'../train/loantime_train.csv' does not exist"
     ]
    }
   ],
   "source": [
    "print(\"begin\")\n",
    "constructBrowseDetailFeature(LOANTIME_TRAIN_FILE, BROWSE_HISTORY_TRAIN_FILE, \n",
    "                           \"../feature/browse_detail_train20181106.csv\")\n",
    "constructBrowseDetailFeature(LOANTIME_TEST_FILE, BROWSE_HISTORY_TEST_FILE, \n",
    "                           \"../feature/browse_detail_test20181106.csv\")\n",
    "print(\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
